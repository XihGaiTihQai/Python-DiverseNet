{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-27T09:46:56.598187Z",
     "iopub.status.busy": "2025-04-27T09:46:56.597798Z",
     "iopub.status.idle": "2025-04-27T12:48:11.704111Z",
     "shell.execute_reply": "2025-04-27T12:48:11.703226Z",
     "shell.execute_reply.started": "2025-04-27T09:46:56.598161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "class RemoteSensingDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir=None, transform=None):\n",
    "        assert os.path.isdir(image_dir), f\"Không tìm thấy folder {image_dir}\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = sorted(os.listdir(self.image_dir))\n",
    "        if label_dir:\n",
    "            assert os.path.isdir(label_dir), f\"Không tìm thấy folder {label_dir}\"\n",
    "            self.labels = sorted(os.listdir(self.label_dir))\n",
    "            assert len(self.images) == len(self.labels), \"Số ảnh và mask phải khớp\"\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.labels:\n",
    "            m_path = os.path.join(self.label_dir, self.labels[idx])\n",
    "            mask = Image.open(m_path).convert(\"L\")\n",
    "            mask = mask.resize((img.shape[1], img.shape[2]), resample=Image.NEAREST)\n",
    "            mask = torch.from_numpy(np.array(mask)).long()\n",
    "        else:\n",
    "            mask = torch.full((img.shape[1], img.shape[2]), 255, dtype=torch.long)\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá IoU\n",
    "def mean_iou(pred, target, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        p = (pred == cls)\n",
    "        t = (target == cls)\n",
    "        inter = (p & t).sum().float()\n",
    "        union = (p | t).sum().float()\n",
    "        if union > 0:\n",
    "            ious.append((inter / union).item())\n",
    "    return np.mean(ious) if ious else 1.0\n",
    "\n",
    "\n",
    "# Mô hình DiverseHead\n",
    "class DiverseHead(nn.Module):\n",
    "    def __init__(self, num_classes, num_heads=10, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.backbone = deeplabv3_resnet50(pretrained=False, num_classes=num_classes)\n",
    "        in_ch = self.backbone.classifier[0].project[0].out_channels\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_ch, 256, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Conv2d(256, num_classes, 1)\n",
    "            ) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone.backbone(x)['out']\n",
    "        feats = self.backbone.classifier[0](feats)\n",
    "        outs = []\n",
    "        for head in self.heads:\n",
    "            o = head(feats)\n",
    "            o = F.interpolate(o, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            outs.append(o)\n",
    "        return outs\n",
    "\n",
    "    def freeze_heads(self, frozen_ids):\n",
    "        for idx, head in enumerate(self.heads):\n",
    "            for p in head.parameters():\n",
    "                p.requires_grad = (idx not in frozen_ids)\n",
    "\n",
    "\n",
    "# Hàm tạo pseudo-label với mean voting + max voting\n",
    "def generate_pseudo_labels_voting(model, dl_unlab, device, num_classes=2, threshold=0.6, phi=2.0):\n",
    "    model.eval()\n",
    "    pseudo_imgs, pseudo_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_u, _ in dl_unlab:\n",
    "            x_u = x_u.to(device)\n",
    "            outs = model(x_u)\n",
    "\n",
    "            probs_heads = [F.softmax(o, dim=1) for o in outs]              # List of [B, C, H, W]\n",
    "            probs_stack = torch.stack(probs_heads, dim=0)                 # [L, B, C, H, W]\n",
    "            probs_mean = probs_stack.mean(dim=0)                          # [B, C, H, W]\n",
    "            mean_label = probs_mean.argmax(dim=1)                         # [B, H, W]\n",
    "\n",
    "            vote_counts = torch.zeros_like(probs_mean, dtype=torch.float)  # [B, C, H, W]\n",
    "            for lbl in [p.argmax(dim=1) for p in probs_heads]:\n",
    "                for c in range(num_classes):\n",
    "                    vote_counts[:, c] += (lbl == c).float()\n",
    "\n",
    "            for c in range(num_classes):\n",
    "                vote_counts[:, c] += phi * (mean_label == c).float()\n",
    "\n",
    "            pseudo = vote_counts.argmax(dim=1)                            # [B, H, W]\n",
    "            max_probs, _ = probs_mean.max(dim=1)                          # [B, H, W]\n",
    "            pseudo[max_probs < threshold] = 255                           # Loại bỏ pixel không tin tưởng\n",
    "\n",
    "            pseudo_imgs.append(x_u.cpu())\n",
    "            pseudo_labels.append(pseudo.cpu())\n",
    "    return torch.cat(pseudo_imgs, dim=0), torch.cat(pseudo_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thais\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thais\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Thiết bị\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 2\n",
    "BS = 4\n",
    "\n",
    "# Các phép biến đổi (augmentation)\n",
    "transform_train = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "transform_val = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "# Đường dẫn dữ liệu\n",
    "train_lab_dir = \"img_test_ver2/img_test_ver2/train_have_label\"\n",
    "train_lab_mask = \"img_test_ver2/img_test_ver2/mask_train_label\"\n",
    "train_unlab_dir = \"img_test_ver2/img_test_ver2/train_unlabel\"\n",
    "val_img_dir = \"img_test_ver2/img_test_ver2/val\"\n",
    "val_mask_dir = \"img_test_ver2/img_test_ver2/val_labels\"\n",
    "\n",
    "# Dataset và DataLoader\n",
    "ds_lab = RemoteSensingDataset(train_lab_dir, train_lab_mask, transform=transform_train)\n",
    "ds_unlab = RemoteSensingDataset(train_unlab_dir, None, transform=transform_train)\n",
    "ds_val = RemoteSensingDataset(val_img_dir, val_mask_dir, transform=transform_val)\n",
    "\n",
    "dl_lab = DataLoader(ds_lab, BS, shuffle=True, num_workers=0, pin_memory=True)\n",
    "dl_unlab = DataLoader(ds_unlab, BS, shuffle=True, num_workers=0, pin_memory=True)\n",
    "dl_val = DataLoader(ds_val, BS, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Khởi tạo mô hình, tối ưu hóa, loss\n",
    "model = DiverseHead(num_classes=num_classes, num_heads=10, dropout_rate=0.3).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase1] Ep 1/20 | loss=0.3965 | Val mIoU=0.8913\n",
      "[Phase1] Ep 2/20 | loss=0.0113 | Val mIoU=0.8913\n",
      "[Phase1] Ep 3/20 | loss=0.0050 | Val mIoU=0.8913\n",
      "[Phase1] Ep 4/20 | loss=0.0023 | Val mIoU=0.8913\n",
      "[Phase1] Ep 5/20 | loss=0.0020 | Val mIoU=0.8913\n",
      "[Phase1] Ep 6/20 | loss=0.0014 | Val mIoU=0.8913\n",
      "[Phase1] Ep 7/20 | loss=0.0014 | Val mIoU=0.8913\n",
      "[Phase1] Ep 8/20 | loss=0.0011 | Val mIoU=0.8913\n",
      "[Phase1] Ep 9/20 | loss=0.0010 | Val mIoU=0.8913\n",
      "[Phase1] Ep 10/20 | loss=0.0009 | Val mIoU=0.8913\n",
      "[Phase1] Ep 11/20 | loss=0.0007 | Val mIoU=0.8913\n",
      "[Phase1] Ep 12/20 | loss=0.0006 | Val mIoU=0.8913\n",
      "[Phase1] Ep 13/20 | loss=0.0007 | Val mIoU=0.8913\n",
      "[Phase1] Ep 14/20 | loss=0.0006 | Val mIoU=0.8913\n",
      "[Phase1] Ep 15/20 | loss=0.0007 | Val mIoU=0.8913\n",
      "[Phase1] Ep 16/20 | loss=0.0006 | Val mIoU=0.8913\n",
      "[Phase1] Ep 17/20 | loss=0.0005 | Val mIoU=0.8913\n",
      "[Phase1] Ep 18/20 | loss=0.0005 | Val mIoU=0.8913\n",
      "[Phase1] Ep 19/20 | loss=0.0003 | Val mIoU=0.8913\n",
      "[Phase1] Ep 20/20 | loss=0.0003 | Val mIoU=0.8913\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Huấn luyện với dữ liệu gán nhãn + không gán nhãn (semi-supervised)\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    it_lab = iter(dl_lab)\n",
    "    it_unlab = iter(dl_unlab)\n",
    "    steps = max(len(dl_lab), len(dl_unlab))\n",
    "\n",
    "    for _ in range(steps):\n",
    "        try:\n",
    "            x_l, y_l = next(it_lab)\n",
    "        except StopIteration:\n",
    "            it_lab = iter(dl_lab)\n",
    "            x_l, y_l = next(it_lab)\n",
    "\n",
    "        try:\n",
    "            x_u, _ = next(it_unlab)\n",
    "        except StopIteration:\n",
    "            it_unlab = iter(dl_unlab)\n",
    "            x_u, _ = next(it_unlab)\n",
    "\n",
    "        x_l, y_l = x_l.to(device), y_l.to(device)\n",
    "        x_u = x_u.to(device)\n",
    "\n",
    "        # Dynamic freezing: đóng băng một nửa số head\n",
    "        frozen = random.sample(range(model.num_heads), model.num_heads // 2)\n",
    "        model.freeze_heads(frozen)\n",
    "\n",
    "        outs_l = model(x_l)  # supervised outputs\n",
    "        outs_u = model(x_u)  # unsupervised outputs\n",
    "\n",
    "        # Loss có label\n",
    "        sup_loss = sum(criterion(o, y_l) for o in outs_l) / model.num_heads\n",
    "\n",
    "        # Loss không có label (pseudo từ trung bình)\n",
    "        probs = torch.stack([F.softmax(o, 1) for o in outs_u], 0).mean(0)\n",
    "        pseudo = probs.argmax(1).detach()\n",
    "        idx = random.randrange(model.num_heads)\n",
    "        unsup_loss = criterion(outs_u[idx], pseudo)\n",
    "\n",
    "        loss = sup_loss + unsup_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.freeze_heads([])\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation sau mỗi epoch\n",
    "    model.eval()\n",
    "    miou_sum, cnt = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_v, y_v in dl_val:\n",
    "            x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "            outs = model(x_v)\n",
    "            out = torch.stack(outs, 0).mean(0).argmax(1)\n",
    "            miou_sum += mean_iou(out.cpu(), y_v.cpu(), num_classes)\n",
    "            cnt += 1\n",
    "    val_miou = miou_sum / cnt\n",
    "    print(f\"[Phase1] Ep {epoch}/{num_epochs} | loss={running_loss/steps:.4f} | Val mIoU={val_miou:.4f}\")\n",
    "\n",
    "# Lưu model sau phase 1\n",
    "torch.save(model.state_dict(), \"diversehead_phase1.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels with voting...\n",
      "Generated 103 pseudo-labeled samples.\n",
      "Saved pseudo-labels to saved_pseudo_labels/\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Tạo pseudo-labels với mean + max voting\n",
    "print(\"Generating pseudo-labels with voting...\")\n",
    "pseudo_imgs, pseudo_labels = generate_pseudo_labels_voting(\n",
    "    model, dl_unlab, device, num_classes=num_classes, threshold=0.6, phi=2.0\n",
    ")\n",
    "print(f\"Generated {pseudo_imgs.shape[0]} pseudo-labeled samples.\")\n",
    "\n",
    "# Lưu pseudo masks dưới dạng ảnh grayscale\n",
    "save_dir = \"saved_pseudo_labels\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i in range(pseudo_labels.shape[0]):\n",
    "    mask = pseudo_labels[i].numpy().astype(np.uint8)\n",
    "    m_pil = Image.fromarray(mask, mode=\"L\")\n",
    "    m_pil.save(os.path.join(save_dir, f\"pseudo_mask_{i:03d}.png\"))\n",
    "print(f\"Saved pseudo-labels to {save_dir}/\")\n",
    "\n",
    "# (Tùy chọn) Augment nhẹ để tăng robustness\n",
    "pseudo_imgs = torch.clamp(pseudo_imgs + 0.01 * torch.randn_like(pseudo_imgs), -1, 1)\n",
    "\n",
    "# Dataset từ pseudo-labels\n",
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, imgs, masks):\n",
    "        self.imgs = imgs\n",
    "        self.masks = masks\n",
    "    def __len__(self): return len(self.imgs)\n",
    "    def __getitem__(self, idx): return self.imgs[idx], self.masks[idx]\n",
    "\n",
    "ds_pseudo = PseudoDataset(pseudo_imgs, pseudo_labels)\n",
    "dl_pseudo = DataLoader(ds_pseudo, BS, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Phase 2: Fine-tune với dữ liệu gán nhãn + pseudo-label\n",
    "ft_epochs = 10\n",
    "for epoch in range(1, ft_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    it_lab = iter(dl_lab)\n",
    "    it_pseudo = iter(dl_pseudo)\n",
    "    steps = max(len(dl_lab), len(dl_pseudo))\n",
    "\n",
    "    for _ in range(steps):\n",
    "        try:\n",
    "            x_l, y_l = next(it_lab)\n",
    "        except StopIteration:\n",
    "            it_lab = iter(dl_lab)\n",
    "            x_l, y_l = next(it_lab)\n",
    "        try:\n",
    "            x_p, y_p = next(it_pseudo)\n",
    "        except StopIteration:\n",
    "            it_pseudo = iter(dl_pseudo)\n",
    "            x_p, y_p = next(it_pseudo)\n",
    "\n",
    "        x_l, y_l = x_l.to(device), y_l.to(device)\n",
    "        x_p, y_p = x_p.to(device), y_p.to(device)\n",
    "\n",
    "        frozen = random.sample(range(model.num_heads), model.num_heads // 2)\n",
    "        model.freeze_heads(frozen)\n",
    "\n",
    "        outs_l = model(x_l)\n",
    "        outs_p = model(x_p)\n",
    "\n",
    "        sup_loss = sum(criterion(o, y_l) for o in outs_l) / model.num_heads\n",
    "        pseudo_loss = sum(criterion(o, y_p) for o in outs_p) / model.num_heads\n",
    "\n",
    "        loss = sup_loss + pseudo_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.freeze_heads([])\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation sau mỗi epoch\n",
    "    model.eval()\n",
    "    miou_sum, cnt = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_v, y_v in dl_val:\n",
    "            x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "            outs = model(x_v)\n",
    "            out = torch.stack(outs, 0).mean(0).argmax(1)\n",
    "            miou_sum += mean_iou(out.cpu(), y_v.cpu(), num_classes)\n",
    "            cnt += 1\n",
    "    val_miou = miou_sum / cnt\n",
    "    print(f\"[Fine-tune] Ep {epoch}/{ft_epochs} | loss={running_loss/steps:.4f} | Val mIoU={val_miou:.4f}\")\n",
    "\n",
    "# Lưu model cuối cùng\n",
    "out_path = \"diversehead_final_selftrain.pth\"\n",
    "torch.save(model.state_dict(), out_path)\n",
    "print(f\"Finished training. Model saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "model.eval()\n",
    "save_pred_dir = \"predictions\"\n",
    "os.makedirs(save_pred_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(dl_val):\n",
    "        x = x.to(device)\n",
    "        outs = model(x)\n",
    "        pred = torch.stack(outs, 0).mean(0).argmax(1)  # [B, H, W]\n",
    "        for b in range(x.size(0)):\n",
    "            mask = pred[b].cpu().numpy().astype(np.uint8) * 127\n",
    "            out_img = Image.fromarray(mask, mode=\"L\")\n",
    "            out_img.save(os.path.join(save_pred_dir, f\"pred_{i*BS + b:03d}.png\"))\n",
    "print(f\"Saved predictions to {save_pred_dir}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7264460,
     "sourceId": 11585843,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7264473,
     "sourceId": 11585863,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
